{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efe1277a-875d-4e88-8453-efc07bff98b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER='E:/sp2023stock/TopicModeling/tmp'\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "%env JOBLIB_TEMP_FOLDER= 'E:/sp2023stock/TopicModeling/tmp'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0fb6aa-f3c7-4d67-988e-d64018c221a2",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9afe5ee8-64ca-440b-83bb-1e73377e7aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\sp2023stock\\\\TopicModeling\\\\src/../model/ModelingAssets/1mill/CorpusDict100'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 100\n",
    "\n",
    "main = os.getcwd()\n",
    "#in\n",
    "#model_path_asset = main + \"/../model/ModelingAssets/CorpusDict\"\n",
    "model_path_asset_1mill = main + \"/../model/ModelingAssets/1mill/CorpusDict\"\n",
    "\n",
    "#out\n",
    "model_path_state = main + \"/../model/ModelingState\"\n",
    "output_result = main + \"/../result/modelOutput\"\n",
    "\n",
    "\n",
    "model_path_asset_spenum = model_path_asset_1mill+str(num)\n",
    "model_path_asset_spenum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e9f4dfc-a49b-4bae-ad4e-2fe4eb397568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loading  dictionary - corpus - json\n",
      "Dataset loaded - dictionary 225206 - corpus 1156987 - json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Dataset loading  dictionary - corpus - json\")\n",
    "# Load the dictionary and corpus\n",
    "dictionary = Dictionary.load(f'{model_path_asset_spenum}/dictionary.gensim')\n",
    "corpus = MmCorpus(f'{model_path_asset_spenum}/corpus.mm')\n",
    "# json_file_path = f'{model_path_asset_spenum}/dataset_{num}.json'\n",
    "\n",
    "# with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "#     dataset = json.load(json_file)\n",
    "    \n",
    "print(f\"Dataset loaded - dictionary {len(dictionary)} - corpus {len(corpus)} - json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d5a3df2-88b0-4259-871c-5a866270603b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the model using corpus: 1156987, iterations: 2000, number of topic: 400\n",
      "Finishing\n",
      "\n",
      "\n",
      "Saving\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_path_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path_state, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlda_model_n\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_topics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_iterations\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_iterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_size100.gensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m lda\u001b[38;5;241m.\u001b[39msave(filename)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved model at pass number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_iterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_path_state' is not defined"
     ]
    }
   ],
   "source": [
    "num_topics = 400\n",
    "total_iterations = 2000\n",
    "save_interval = 100\n",
    "\n",
    "# Initialize LDA model\n",
    "\n",
    "print(f\"Running the model using corpus: {len(corpus)}, iterations: {total_iterations}, number of topic: {num_topics}\")\n",
    "\n",
    "lda = gensim.models.ldamulticore.LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    iterations= total_iterations\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"Finishing\")\n",
    "print(\"\\n\")\n",
    "print(\"Saving\")\n",
    "\n",
    "filename = os.path.join(model_path_state, f'lda_model_n{num_topics}_iterations{total_iterations}_size100.gensim')\n",
    "lda.save(filename)\n",
    "print(f\"Saved model at pass number {total_iterations} to {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16412849-d236-43dc-a592-b9f4da4230cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model at pass number 2000 to E:\\sp2023stock\\TopicModeling\\src/../model/ModelingState\\lda_model_n400_iterations2000_size100.gensim\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(model_path_state, f'lda_model_n{num_topics}_iterations{total_iterations}_size100.gensim')\n",
    "lda.save(filename)\n",
    "print(f\"Saved model at pass number {total_iterations} to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d662ba8-d99a-485f-afb3-267792487834",
   "metadata": {},
   "source": [
    "#### Loading (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e49108e-e2bb-4821-a717-37a1fe901694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model Completed\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(model_path_state, f'lda_model_n400_iterations2000_size100.gensim')\n",
    "lda = LdaModel.load(filename)\n",
    "print(f\"Load model Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5084910d-f4fa-4210-8e6e-7281552c34e9",
   "metadata": {},
   "source": [
    "#### Inferanceing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca209da-81a6-464d-af9c-6713fa1850a0",
   "metadata": {},
   "source": [
    "###### Print Distribution of each articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1c962cf-5898-4af0-be5d-c2dfc17fd949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distribution for each document\n",
    "doc_topic_dists = [lda.get_document_topics(item, minimum_probability=0) for item in corpus]\n",
    "\n",
    "# Convert the topic distributions into a structured format\n",
    "doc_topics = []\n",
    "for doc_dist in doc_topic_dists:\n",
    "    doc_topics.append([prob for _, prob in doc_dist])\n",
    "\n",
    "# Create a DataFrame\n",
    "doc_topics_df = pd.DataFrame(doc_topics, columns=[f\"Topic_{i}\" for i in range(num_topics)])\n",
    "\n",
    "# # If you have a list of article IDs, you can insert them too\n",
    "# doc_topics_df.insert(0, 'Article_ID', processed_article_ids)\n",
    "doc_topic_out = os.path.join(output_result, 'document_topics.csv') \n",
    "doc_topics_df.to_csv(doc_topic_out, index=False)\n",
    "\n",
    "print(f\"Topic distribution for each article is completely saved at {doc_topic_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c9592-f66e-4eb5-ba88-746dde36d490",
   "metadata": {},
   "source": [
    "###### Print Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bad2200f-2acb-48e0-89e9-e26ecf70bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered Topics:\n",
      "Topics saved to E:\\sp2023stock\\TopicModeling\\src/../result/modelOutput\\word_topics.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the topics data\n",
    "topics_data = []\n",
    "num_topics = 400\n",
    "num_showwords = 100\n",
    "print(f\"Total of words in each topic are{num_showwords}\") \n",
    "\n",
    "for i in range(num_topics):\n",
    "    words = lda.show_topic(i, num_showwords)\n",
    "    topic_words = [word[0] for word in words]\n",
    "    topics_data.append(topic_words)\n",
    "\n",
    "# Create a DataFrame from the topics data\n",
    "df_topics = pd.DataFrame(topics_data, index=[f\"Topic {i}\" for i in range(num_topics)], columns=[f\"Word {i+1}\" for i in range(num_showwords)])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = os.path.join(output_result, 'word_topics.csv')\n",
    "df_topics.to_csv(csv_file_path, index=True)\n",
    "\n",
    "print(f\"Words for each topic is completely saved at {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37be90e-71df-433f-bc10-1d963639baa0",
   "metadata": {},
   "source": [
    "#### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07389b51-c6d7-4a6e-a698-6807424249fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>Topic_4</th>\n",
       "      <th>Topic_5</th>\n",
       "      <th>Topic_6</th>\n",
       "      <th>Topic_7</th>\n",
       "      <th>Topic_8</th>\n",
       "      <th>Topic_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Topic_390</th>\n",
       "      <th>Topic_391</th>\n",
       "      <th>Topic_392</th>\n",
       "      <th>Topic_393</th>\n",
       "      <th>Topic_394</th>\n",
       "      <th>Topic_395</th>\n",
       "      <th>Topic_396</th>\n",
       "      <th>Topic_397</th>\n",
       "      <th>Topic_398</th>\n",
       "      <th>Topic_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156982</th>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156983</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.057179</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156984</th>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.386651</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156985</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156986</th>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1156987 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Topic_0   Topic_1   Topic_2   Topic_3   Topic_4   Topic_5   Topic_6  \\\n",
       "0        0.000104  0.000104  0.000104  0.000104  0.000104  0.000104  0.000104   \n",
       "1        0.000071  0.000071  0.000071  0.000071  0.000071  0.000071  0.000071   \n",
       "2        0.000125  0.000125  0.000125  0.000125  0.000125  0.000125  0.000125   \n",
       "3        0.000042  0.000042  0.000042  0.000042  0.000042  0.000042  0.000042   \n",
       "4        0.000038  0.000038  0.000038  0.000038  0.000038  0.000038  0.000038   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1156982  0.000076  0.000076  0.000076  0.000076  0.000076  0.000076  0.000076   \n",
       "1156983  0.000031  0.000031  0.057179  0.000031  0.000031  0.000031  0.000031   \n",
       "1156984  0.000046  0.000046  0.000046  0.000046  0.000046  0.000046  0.000046   \n",
       "1156985  0.000026  0.000026  0.000026  0.000026  0.000026  0.000026  0.000026   \n",
       "1156986  0.000069  0.000069  0.000069  0.000069  0.000069  0.000069  0.000069   \n",
       "\n",
       "          Topic_7   Topic_8   Topic_9  ...  Topic_390  Topic_391  Topic_392  \\\n",
       "0        0.000104  0.000104  0.000104  ...   0.000104   0.000104   0.000104   \n",
       "1        0.000071  0.000071  0.000071  ...   0.000071   0.000071   0.000071   \n",
       "2        0.000125  0.000125  0.000125  ...   0.000125   0.000125   0.000125   \n",
       "3        0.000042  0.000042  0.000042  ...   0.000042   0.000042   0.000042   \n",
       "4        0.000038  0.000038  0.000038  ...   0.000038   0.000038   0.000038   \n",
       "...           ...       ...       ...  ...        ...        ...        ...   \n",
       "1156982  0.000076  0.000076  0.000076  ...   0.000076   0.000076   0.000076   \n",
       "1156983  0.000031  0.000031  0.000031  ...   0.000031   0.000031   0.000031   \n",
       "1156984  0.000046  0.000046  0.000046  ...   0.000046   0.000046   0.000046   \n",
       "1156985  0.000026  0.000026  0.000026  ...   0.000026   0.000026   0.000026   \n",
       "1156986  0.000069  0.000069  0.000069  ...   0.000069   0.000069   0.000069   \n",
       "\n",
       "         Topic_393  Topic_394  Topic_395  Topic_396  Topic_397  Topic_398  \\\n",
       "0         0.000104   0.000104   0.000104   0.000104   0.000104   0.000104   \n",
       "1         0.000071   0.000071   0.000071   0.000071   0.000071   0.000071   \n",
       "2         0.000125   0.000125   0.000125   0.000125   0.000125   0.000125   \n",
       "3         0.000042   0.000042   0.000042   0.000042   0.000042   0.000042   \n",
       "4         0.000038   0.000038   0.000038   0.000038   0.000038   0.000038   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "1156982   0.000076   0.000076   0.000076   0.000076   0.000076   0.000076   \n",
       "1156983   0.000031   0.000031   0.000031   0.000031   0.000031   0.000031   \n",
       "1156984   0.000046   0.000046   0.000046   0.000046   0.000046   0.386651   \n",
       "1156985   0.000026   0.000026   0.000026   0.000026   0.000026   0.000026   \n",
       "1156986   0.000069   0.000069   0.000069   0.000069   0.000069   0.000069   \n",
       "\n",
       "         Topic_399  \n",
       "0         0.000104  \n",
       "1         0.000071  \n",
       "2         0.000125  \n",
       "3         0.000042  \n",
       "4         0.000038  \n",
       "...            ...  \n",
       "1156982   0.000076  \n",
       "1156983   0.000031  \n",
       "1156984   0.000046  \n",
       "1156985   0.000026  \n",
       "1156986   0.000069  \n",
       "\n",
       "[1156987 rows x 400 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(doc_topic_out)\n",
    "len(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61378115-7852-45f6-a0bf-9ba633d72987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Agency</th>\n",
       "      <th>Date</th>\n",
       "      <th>Links</th>\n",
       "      <th>Title</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>prachachat</td>\n",
       "      <td>09/06/2017 09:49:48</td>\n",
       "      <td>https://www.prachachat.net/finance/news-13</td>\n",
       "      <td>จองพื้นที่มอเตอร์เอ็กซ์โปคึก เชื่อยอดขายพุ่ง</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>prachachat</td>\n",
       "      <td>09/06/2017 10:12:16</td>\n",
       "      <td>https://www.prachachat.net/finance/news-20</td>\n",
       "      <td>ทอท.ปรับดอนเมืองเฟส 3 เพิ่มระบบ APM-ชงบอร์ด พ....</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>prachachat</td>\n",
       "      <td>09/06/2017 10:15:58</td>\n",
       "      <td>https://www.prachachat.net/finance/news-21</td>\n",
       "      <td>\"คีรี\" รีเทิร์นรอบ 25 ปีลงทุนรถไฟฟ้า เราเคยบาด...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>prachachat</td>\n",
       "      <td>09/06/2017 10:16:00</td>\n",
       "      <td>https://www.prachachat.net/finance/news-22</td>\n",
       "      <td>อัดขายบ้านสายสีม่วงลุ้น Q2 เชื่อมแน่สถานีเตาปู...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>prachachat</td>\n",
       "      <td>09/06/2017 10:16:04</td>\n",
       "      <td>https://www.prachachat.net/finance/news-25</td>\n",
       "      <td>ธปท.ผ่อนคุมแลกเงิน-เพิ่มผู้เล่น เอื้อธุรกิจ-รา...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID      Agency                 Date  \\\n",
       "0  13  prachachat  09/06/2017 09:49:48   \n",
       "1  20  prachachat  09/06/2017 10:12:16   \n",
       "2  21  prachachat  09/06/2017 10:15:58   \n",
       "3  22  prachachat  09/06/2017 10:16:00   \n",
       "4  25  prachachat  09/06/2017 10:16:04   \n",
       "\n",
       "                                        Links  \\\n",
       "0  https://www.prachachat.net/finance/news-13   \n",
       "1  https://www.prachachat.net/finance/news-20   \n",
       "2  https://www.prachachat.net/finance/news-21   \n",
       "3  https://www.prachachat.net/finance/news-22   \n",
       "4  https://www.prachachat.net/finance/news-25   \n",
       "\n",
       "                                               Title  Status  \n",
       "0       จองพื้นที่มอเตอร์เอ็กซ์โปคึก เชื่อยอดขายพุ่ง    True  \n",
       "1  ทอท.ปรับดอนเมืองเฟส 3 เพิ่มระบบ APM-ชงบอร์ด พ....    True  \n",
       "2  \"คีรี\" รีเทิร์นรอบ 25 ปีลงทุนรถไฟฟ้า เราเคยบาด...    True  \n",
       "3  อัดขายบ้านสายสีม่วงลุ้น Q2 เชื่อมแน่สถานีเตาปู...    True  \n",
       "4  ธปท.ผ่อนคุมแลกเงิน-เพิ่มผู้เล่น เอื้อธุรกิจ-รา...    True  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masterFileasd = 'E:/sp2023stock/TopicModeling/data/ModelingDataset/Test/V.2/MasterDesctiption.csv'\n",
    "masterdf = pd.read_csv(masterFileasd, nrows=5)\n",
    "masterdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ff023f1-d1e2-4db4-b25c-fa4b92706626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataFile = 'E:/sp2023stock/TopicModeling/data/ModelingDataset/Test/V.2/dataset.csv'\n",
    "datadf = pd.read_csv(dataFile)\n",
    "datadf = datadf.drop('Article', axis=1)\n",
    "datadf.to_csv('E:/sp2023stock/TopicModeling/data/ModelingDataset/Test/V.2/dateonly.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "74e68fa3-b44f-4cac-a95a-6c2c749e6861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1156987"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateonly_p = 'E:/sp2023stock/TopicModeling/data/ModelingDataset/Test/V.2/dateonly.csv'\n",
    "dateonly = pd.read_csv(dateonly_p)\n",
    "len(dateonly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "75ce07f2-a350-4377-9ee3-1eb7508b9339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Agency</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>prachachat</td>\n",
       "      <td>2017-06-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>prachachat</td>\n",
       "      <td>2017-06-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>prachachat</td>\n",
       "      <td>2017-06-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>prachachat</td>\n",
       "      <td>2017-06-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>prachachat</td>\n",
       "      <td>2017-06-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156982</th>\n",
       "      <td>1042510</td>\n",
       "      <td>dailynews</td>\n",
       "      <td>2022-05-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156983</th>\n",
       "      <td>1042584</td>\n",
       "      <td>dailynews</td>\n",
       "      <td>2022-05-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156984</th>\n",
       "      <td>1042589</td>\n",
       "      <td>dailynews</td>\n",
       "      <td>2022-05-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156985</th>\n",
       "      <td>1042592</td>\n",
       "      <td>dailynews</td>\n",
       "      <td>2022-05-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156986</th>\n",
       "      <td>1042617</td>\n",
       "      <td>dailynews</td>\n",
       "      <td>2022-05-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1156987 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID      Agency        Date\n",
       "0             13  prachachat  2017-06-09\n",
       "1             20  prachachat  2017-06-09\n",
       "2             21  prachachat  2017-06-09\n",
       "3             22  prachachat  2017-06-09\n",
       "4             25  prachachat  2017-06-09\n",
       "...          ...         ...         ...\n",
       "1156982  1042510   dailynews  2022-05-14\n",
       "1156983  1042584   dailynews  2022-05-12\n",
       "1156984  1042589   dailynews  2022-05-12\n",
       "1156985  1042592   dailynews  2022-05-12\n",
       "1156986  1042617   dailynews  2022-05-12\n",
       "\n",
       "[1156987 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateonly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "55d53fcb-4f9b-45bb-bc78-caaf1803dfc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'method' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Merging\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(doc_topic_out) \u001b[38;5;66;03m#top 3\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dateonly) \u001b[38;5;66;03m# master\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df1) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(df2)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m df2\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:704\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    701\u001b[0m errors \u001b[38;5;241m=\u001b[39m errors \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# read_csv does not know whether the buffer is opened in binary/text mode\u001b[39;00m\n\u001b[1;32m--> 704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_binary_mode(path_or_buf, mode) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    705\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;66;03m# validate encoding and errors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:1163\u001b[0m, in \u001b[0;36m_is_binary_mode\u001b[1;34m(handle, mode)\u001b[0m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(handle), text_classes):\n\u001b[0;32m   1161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, _get_binary_io_classes()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m   1164\u001b[0m     handle, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\n\u001b[0;32m   1165\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'method' is not iterable"
     ]
    }
   ],
   "source": [
    "# Merging\n",
    "df1 = pd.read_csv(doc_topic_out) #top 3\n",
    "df2 = pd.read_csv(dateonly) # master\n",
    "\n",
    "assert len(df1) == len(df2)\n",
    "\n",
    "for column in df2.columns:\n",
    "    df1[column] = df2[column].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35548bd7-1a73-4f28-aeee-b14ccd4b289a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
