{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe1277a-875d-4e88-8453-efc07bff98b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER='E:/sp2023stock/TopicModeling/tmp'\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "%env JOBLIB_TEMP_FOLDER= 'E:/sp2023stock/TopicModeling/tmp'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9afe5ee8-64ca-440b-83bb-1e73377e7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 100\n",
    "\n",
    "main = os.getcwd()\n",
    "model_path_asset = main + \"/../model/ModelingAssets/CorpusDict\"\n",
    "model_path_asset_spenum = model_path_asset+str(num)\n",
    "\n",
    "model_path_state = main + \"/../model/ModelingState\"\n",
    "output_reuslt = main + \"/../result/RawOuput\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e9f4dfc-a49b-4bae-ad4e-2fe4eb397568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loading  dictionary - corpus - json\n",
      "Dataset loaded - dictionary 137788 - corpus 506010 - json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Dataset loading  dictionary - corpus - json\")\n",
    "# Load the dictionary and corpus\n",
    "dictionary = Dictionary.load(f'{model_path_asset_spenum}/dictionary.gensim')\n",
    "corpus = MmCorpus(f'{model_path_asset_spenum}/corpus.mm')\n",
    "# json_file_path = f'{model_path_asset_spenum}/dataset_{num}.json'\n",
    "\n",
    "# with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "#     dataset = json.load(json_file)\n",
    "    \n",
    "print(f\"Dataset loaded - dictionary {len(dictionary)} - corpus {len(corpus)} - json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a3df2-88b0-4259-871c-5a866270603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 200\n",
    "total_passes = 500\n",
    "save_interval = 100\n",
    "\n",
    "# Initialize LDA model\n",
    "lda = gensim.models.ldamulticore.LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "interval_start_time = time.time()  # Start time for the interval\n",
    "\n",
    "for pass_num in range(total_passes):\n",
    "    lda.update(corpus)\n",
    "\n",
    "    if (pass_num + 1) % save_interval == 0:\n",
    "        filename = os.path.join(model_path_state, f'lda_model_n{num_topics}_passes{pass_num+1}_size{num}.gensim')\n",
    "        lda.save(filename)\n",
    "        print(f\"Saved model at pass number {pass_num + 1} to {filename}\")\n",
    "\n",
    "        interval_end_time = time.time()  # End time for the interval\n",
    "        time_for_interval = interval_end_time - interval_start_time\n",
    "        print(f\"Time taken for passes {pass_num + 1 - save_interval} to {pass_num + 1}: {time_for_interval} seconds\")\n",
    "\n",
    "        interval_start_time = time.time()  # Reset start time for the next interval\n",
    "\n",
    "# Print total time taken after all passes\n",
    "total_end_time = time.time()\n",
    "total_time = total_end_time - interval_start_time\n",
    "print(f\"Total time taken for all passes: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c962cf-5898-4af0-be5d-c2dfc17fd949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distribution for each document\n",
    "doc_topic_dists = [lda.get_document_topics(item, minimum_probability=0) for item in corpus]\n",
    "\n",
    "# Convert the topic distributions into a structured format\n",
    "doc_topics = []\n",
    "for doc_dist in doc_topic_dists:\n",
    "    doc_topics.append([prob for _, prob in doc_dist])\n",
    "\n",
    "# Create a DataFrame\n",
    "doc_topics_df = pd.DataFrame(doc_topics, columns=[f\"Topic_{i}\" for i in range(num_topics)])\n",
    "\n",
    "# If you have a list of article IDs, you can insert them too\n",
    "doc_topics_df.insert(0, 'Article_ID', processed_article_ids)\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "doc_topics_df.to_csv(f\"output_reuslt/document_topics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2200f-2acb-48e0-89e9-e26ecf70bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the topics data\n",
    "topics_data = []\n",
    "\n",
    "# Retrieve the topics and their words\n",
    "print(\"Discovered Topics:\")\n",
    "for i in range(num_topics):\n",
    "    words = lda.show_topic(i, 25)\n",
    "    topic_words = [word[0] for word in words]\n",
    "    topics_data.append(topic_words)\n",
    "    print(f\"Topic {i}:\", topic_words)\n",
    "\n",
    "# Create a DataFrame from the topics data\n",
    "df_topics = pd.DataFrame(topics_data, index=[f\"Topic {i}\" for i in range(num_topics)], columns=[f\"Word {i+1}\" for i in range(25)])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = os.path.join(output_result, 'word_topics.csv')\n",
    "df_topics.to_csv(csv_file_path, index=True)\n",
    "\n",
    "print(f\"Topics saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1837a88-e5c5-4242-938e-3fdb6dca2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Setting environment variable for JOBLIB\n",
    "os.environ['JOBLIB_TEMP_FOLDER'] = 'E:/sp2023stock/TopicModeling/tmp'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get topic distribution for each document\n",
    "doc_topic_dists = [lda.get_document_topics(item, minimum_probability=0) for item in corpus]\n",
    "\n",
    "# Convert the topic distributions into a structured format\n",
    "doc_topics = [[prob for _, prob in doc_dist] for doc_dist in doc_topic_dists]\n",
    "\n",
    "# Create a DataFrame\n",
    "doc_topics_df = pd.DataFrame(doc_topics, columns=[f\"Topic_{i}\" for i in range(num_topics)])\n",
    "\n",
    "# If you have a list of article IDs, you can insert them too\n",
    "# Ensure that 'processed_article_ids' is defined and has the same length as 'doc_topics'\n",
    "# doc_topics_df.insert(0, 'Article_ID', processed_article_ids)\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "doc_topics_df.to_csv(os.path.join(output_result, \"document_topics.csv\"), index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
