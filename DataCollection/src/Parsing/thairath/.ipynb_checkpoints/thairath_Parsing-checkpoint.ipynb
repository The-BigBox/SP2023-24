{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd5e879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index TXT Processing ID: 2346991\n",
      "Index TXT Processing ID: 2346992\n",
      "Index TXT Processing ID: 2346994\n",
      "Index TXT Processing ID: 2346995\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import chardet\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Detect input file\n",
    "def htmlInput(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        encoding_result = chardet.detect(file.read())\n",
    "    with open(file_path, \"r\", encoding=encoding_result[\"encoding\"]) as file:\n",
    "        html_content = file.read()\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "#Extract title\n",
    "def extract_title(soup):\n",
    "    title = soup.title.string.strip() if soup.title else None\n",
    "    return title\n",
    "\n",
    "#Extract body\n",
    "def extract_body(soup):\n",
    "    article_divs = soup.find_all(\"div\", itemprop=\"articleBody\")\n",
    "    articles = []\n",
    "    if article_divs:\n",
    "        for inside_div in article_divs:\n",
    "            articleses = inside_div.find_all('p')\n",
    "            for article in articleses:\n",
    "                articles.append(article.get_text(strip=True))\n",
    "    return articles\n",
    "\n",
    "#Extract tags\n",
    "def extract_tags(soup):\n",
    "    tags_div = soup.find(\"div\", class_=\"__item_article-breadcrumb css-7obyzk e1wlf1s66\")\n",
    "    tag_content = [tag.get_text(strip=True) for tag in tags_div.find_all('a')] if tags_div else None\n",
    "    return tag_content\n",
    "\n",
    "#Extract date\n",
    "def extract_pubdate(soup):\n",
    "    month_list = {'ม.ค.': '01', 'ก.พ.': '02', 'มี.ค.': '03', 'เม.ย.': '04', 'พ.ค.': '05', 'มิ.ย.': '06', 'ก.ค.': '07', 'ส.ค.': '08', 'ก.ย.': '09', 'ต.ค.': '10', 'พ.ย.': '11', 'ธ.ค.': '12'}\n",
    "    date_time_div = soup.find('div', {'class': '__item_article-date css-1v3en5e e1wlf1s65'})\n",
    "    \n",
    "    datestamps = \"\"\n",
    "    timestamps = \"\"\n",
    "    year = \"\"\n",
    "    numMonth = \"\"\n",
    "    date = \"\"\n",
    "\n",
    "    if date_time_div is not None:\n",
    "        date_time_string = date_time_div.get_text(strip=True)\n",
    "        datestamps = \" \".join(date_time_string.split())\n",
    "        timestamps = \" \".join(date_time_string.split()[3:4])\n",
    "\n",
    "        for month_dict in month_list:\n",
    "            if month_dict in datestamps:\n",
    "                month = month_dict\n",
    "                numMonth = month_list[str(month)]\n",
    "                date = \" \".join(date_time_string.split()[:1])\n",
    "                year = \" \".join(date_time_string.split()[2:3])\n",
    "\n",
    "        if year != \"\":\n",
    "            year = str(int(year) - 543)\n",
    "        datestamps = year + \"-\" + numMonth + \"-\" + date\n",
    "        pubdate = datestamps + \" \" + timestamps\n",
    "    else:\n",
    "        pubdate = None\n",
    "\n",
    "    return pubdate\n",
    "\n",
    "#Extract title\n",
    "def extract_intro(soup):\n",
    "    intro_div = soup.find(\"div\", class_=\"css-1wn93q2 evs3ejl67\")\n",
    "    intro = \"\"\n",
    "    if intro_div:\n",
    "        intro = intro_div.get_text(strip=True)\n",
    "    else:\n",
    "        intro = None\n",
    "    \n",
    "    return intro\n",
    "def extract_urlPic(soup):\n",
    "    url = None\n",
    "    return url\n",
    "\n",
    "#start using \"csv_progress\" (current progress of parsing)\n",
    "if os.path.exists('csv_progress.txt'):\n",
    "    with open('csv_progress.txt','r') as f:\n",
    "        progress_start = int(f.readline().strip())\n",
    "        start_id = progress_start - 1\n",
    "else: \n",
    "    start_id = 1 \n",
    "    \n",
    "article_id = start_id\n",
    "\n",
    "\n",
    "process = '/Users/macintoshhd/Documents/sp2023/DataSet/thairath/progress.txt'\n",
    "\n",
    "#start using \"progress\" (last progress of crwaling)\n",
    "if os.path.exists(process):\n",
    "    with open(process,'r') as f:\n",
    "        progress_end = int(f.readline().strip())\n",
    "        end_id = progress_end + 1\n",
    "\n",
    "for article_id in range(start_id, end_id+1):\n",
    "    file_path = \"/Users/macintoshhd/Documents/sp2023/DataSet/thairath/article/\" + str(article_id) + \"/index.txt\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        print(\"Index TXT Processing ID: \" + str(article_id))\n",
    "        \n",
    "        soup = htmlInput(file_path)\n",
    "        title = extract_title(soup)\n",
    "        intro = extract_intro(soup)\n",
    "        articles = extract_body(soup)\n",
    "        pubdate = extract_pubdate(soup)\n",
    "        tag_content = extract_tags(soup)\n",
    "        url = extract_urlPic(soup)\n",
    "        \n",
    "        data_dict = {'Title': title, 'Intro': intro, 'Article': articles, 'DateTime': pubdate, 'Tags': tag_content, 'url_picture': url}\n",
    "        \n",
    "        with open(\"/Users/macintoshhd/Documents/sp2023/DataSet/thairath/article/\" + str(article_id) + \"/parsed.txt\", 'w', encoding=\"utf-8\") as f:\n",
    "            for key, value in data_dict.items():\n",
    "                if value is not None:\n",
    "                    if key == 'Tags' and isinstance(value, list):\n",
    "                        f.write(f\"[::{key}::]\\n\")\n",
    "                        for tags in value:\n",
    "                            f.write(f\"{tags}\\n\")\n",
    "                            \n",
    "                    elif key == 'Article' and isinstance(value, list):\n",
    "                        f.write(f\"[::{key}::]\\n\")\n",
    "                        for bodys in value:\n",
    "                            if bodys == 'SPONSORED':\n",
    "                                continue\n",
    "                            else:\n",
    "                                f.write(f\"{bodys}\\n\")\n",
    "                                f.write(\"\\n\")\n",
    "                    else:\n",
    "                        f.write(f\"[::{key}::]\\n{value}\\n\")\n",
    "\n",
    "\n",
    "# Write the last processed article_id to 'csv_progress.txt' after loop ends\n",
    "with open('csv_progress.txt', 'w') as f:\n",
    "    f.write(str(article_id) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68400e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
