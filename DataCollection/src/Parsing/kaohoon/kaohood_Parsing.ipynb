{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa28d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17149\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the directory containing the CSV files\n",
    "directory_path = '/data1-8tb-blk/sp2023stock/Insight-Wave/TopicModeling/result/integrationOutput/id/kaohoon_ids.csv'\n",
    "\n",
    "articles_list = {}\n",
    "\n",
    "df = pd.read_csv(directory_path)\n",
    "articles_list = df['ID'].tolist()\n",
    "\n",
    "print(len(articles_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22081c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article 612723: 'NoneType' object has no attribute 'find'\n",
      "Error processing article 612727: 'NoneType' object has no attribute 'find'\n",
      "Error processing article 612732: 'NoneType' object has no attribute 'find'\n",
      "Error processing article 612737: 'NoneType' object has no attribute 'find'\n",
      "Error processing article 612747: 'NoneType' object has no attribute 'find'\n",
      "Error processing article 612754: 'NoneType' object has no attribute 'find'\n",
      "Error processing article 612760: 'NoneType' object has no attribute 'find'\n",
      "Error processing article 612762: 'NoneType' object has no attribute 'find'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 98\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file_path):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m         soup \u001b[38;5;241m=\u001b[39m \u001b[43mhtmlInput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m         title \u001b[38;5;241m=\u001b[39m extract_title(soup)\n\u001b[0;32m    100\u001b[0m         intro \u001b[38;5;241m=\u001b[39m extract_intro(soup)\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36mhtmlInput\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhtmlInput\u001b[39m(file_path):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m---> 12\u001b[0m         encoding_result \u001b[38;5;241m=\u001b[39m \u001b[43mchardet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39mencoding_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     14\u001b[0m         html_content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\SPJ-2023\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\chardet\\__init__.py:49\u001b[0m, in \u001b[0;36mdetect\u001b[1;34m(byte_str, should_rename_legacy)\u001b[0m\n\u001b[0;32m     47\u001b[0m     byte_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(byte_str)\n\u001b[0;32m     48\u001b[0m detector \u001b[38;5;241m=\u001b[39m UniversalDetector(should_rename_legacy\u001b[38;5;241m=\u001b[39mshould_rename_legacy)\n\u001b[1;32m---> 49\u001b[0m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m detector\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\SPJ-2023\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\chardet\\universaldetector.py:236\u001b[0m, in \u001b[0;36mUniversalDetector.feed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_utf1632_prober \u001b[38;5;241m=\u001b[39m UTF1632Prober()\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_utf1632_prober\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m ProbingState\u001b[38;5;241m.\u001b[39mDETECTING:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_utf1632_prober\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_str\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m ProbingState\u001b[38;5;241m.\u001b[39mFOUND_IT:\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_utf1632_prober\u001b[38;5;241m.\u001b[39mcharset_name,\n\u001b[0;32m    239\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_utf1632_prober\u001b[38;5;241m.\u001b[39mget_confidence(),\n\u001b[0;32m    240\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    241\u001b[0m         }\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SPJ-2023\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\chardet\\utf1632prober.py:192\u001b[0m, in \u001b[0;36mUTF1632Prober.feed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquad[mod4] \u001b[38;5;241m=\u001b[39m c\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mod4 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_utf32_characters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_utf16_characters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquad[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_utf16_characters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquad[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m4\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\SPJ-2023\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\chardet\\utf1632prober.py:133\u001b[0m, in \u001b[0;36mUTF1632Prober.validate_utf32_characters\u001b[1;34m(self, quad)\u001b[0m\n\u001b[0;32m    124\u001b[0m     approx_chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapprox_16bit_chars()\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m approx_chars \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_CHARS_FOR_DETECTION \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    126\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonzeros_at_mod[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonzeros_at_mod[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m/\u001b[39m approx_chars\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEXPECTED_RATIO\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvalid_utf16le\n\u001b[0;32m    131\u001b[0m     )\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_utf32_characters\u001b[39m(\u001b[38;5;28mself\u001b[39m, quad: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    Validate if the quad of bytes is valid UTF-32.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    https://en.wikipedia.org/wiki/UTF-32\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    143\u001b[0m         quad[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m quad[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0x10\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (quad[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m quad[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0xD8\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m quad[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0xDF\u001b[39m)\n\u001b[0;32m    146\u001b[0m     ):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def htmlInput(file_path):\n",
    "    with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "#Extract title\n",
    "def extract_title(soup):\n",
    "    title = soup.title.string.strip() if soup.title else None\n",
    "    return title\n",
    "\n",
    "#Extract body\n",
    "def extract_body(soup):\n",
    "    article_divs = soup.find_all(\"div\", class_=\"entry-content entry clearfix\")\n",
    "    articles = []\n",
    "    if article_divs:\n",
    "        for article_div in article_divs:\n",
    "            articleses = article_div.find_all('p')\n",
    "            for article in articleses:\n",
    "                articles.append(\" \".join(article.get_text().split()))\n",
    "    \n",
    "    return articles\n",
    "\n",
    "#Extract tags\n",
    "def extract_tags(soup):\n",
    "    tagcloud_span = soup.find('span', class_='tagcloud')\n",
    "    if tagcloud_span is not None:\n",
    "        tags = tagcloud_span.find_all('a')\n",
    "        tag_content = [tag.get_text(strip=True) for tag in tags]\n",
    "        unnecessary_tag = soup.find('div', class_='post-bottom-meta post-bottom-tags post-tags-classic')\n",
    "        unnecessary_tag.decompose()\n",
    "    else:\n",
    "        tag_content = ['Tag not found']\n",
    "    return tag_content\n",
    "\n",
    "#Extract date\n",
    "def extract_pubdate(soup):\n",
    "    date_div = soup.find('div', {'id': 'single-post-meta'})\n",
    "    date_span = date_div.find('span', {'class': 'date meta-item tie-icon'})\n",
    "    timestamps = date_span.text.strip() if date_span else None\n",
    "    if timestamps is not None:\n",
    "        date_obj = datetime.datetime.strptime(timestamps, '%d/%m/%Y')\n",
    "        pubdate = date_obj.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        pubdate = None\n",
    "    return pubdate\n",
    "\n",
    "#Extract title\n",
    "def extract_intro(soup):\n",
    "    intro_div = soup.find(\"h2\", class_=\"entry-sub-title\")\n",
    "    intro = \"\"\n",
    "    if intro_div:\n",
    "        intro = intro_div.get_text(strip=True)\n",
    "    else:\n",
    "        intro = None\n",
    "\n",
    "    return intro\n",
    "\n",
    "def extract_urlPic(soup):\n",
    "    picture_div = soup.find('figure', class_=\"single-featured-image\")\n",
    "    if picture_div:\n",
    "        img_tag = picture_div.find('img')\n",
    "        if img_tag:\n",
    "            image_link = img_tag.get('src')\n",
    "            print(image_link)\n",
    "        else:\n",
    "            print(\"No img tag found inside the figure\")\n",
    "    else:\n",
    "        print(\"No figure with class 'single-featured-image' found\")\n",
    "    \n",
    "    return url\n",
    "\n",
    "#start using \"csv_progress\" (current progress of parsing) 612723\n",
    "if os.path.exists('parsing_progress.txt'):\n",
    "    with open('parsing_progress.txt','r') as f:\n",
    "        progress_start = int(f.readline().strip())\n",
    "        start_id = progress_start - 1\n",
    "else: \n",
    "    start_id = 1 \n",
    "    \n",
    "article_id = start_id\n",
    "\n",
    "process = 'C:/Users/SPJ-2023/Documents/sp2023/CrawlingCode/kaohoon/progress.txt'\n",
    "\n",
    "#start using \"progress\" (last progress of crwaling) \n",
    "if os.path.exists(process):\n",
    "    with open(process,'r') as f:\n",
    "        progress_end = int(f.readline().strip())\n",
    "        end_id = progress_end + 1\n",
    "\n",
    "\n",
    "for article_id in range(start_id, end_id+1):\n",
    "    file_path = \"C:/Users/SPJ-2023/Documents/sp2023/Dataset/New_crawling/kaohoon/\" + str(article_id) + \"/index.txt\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            soup = htmlInput(file_path)\n",
    "            title = extract_title(soup)\n",
    "            intro = extract_intro(soup)\n",
    "            articles = extract_body(soup)\n",
    "            pubdate = extract_pubdate(soup)\n",
    "            tag_content = extract_tags(soup)\n",
    "            url = extract_urlPic(soup)\n",
    "\n",
    "            data_dict = {'Title': title, 'Intro': intro, 'Article': articles, 'DateTime': pubdate, 'Tags': tag_content, 'url_picture': url}\n",
    "\n",
    "            with open(os.path.join(\"C:/Users/SPJ-2023/Documents/sp2023/Dataset/New_crawling/kaohoon/\" + str(article_id) +\"/parsing.txt\"), 'w', encoding=\"utf-8\") as f:\n",
    "                for key, value in data_dict.items():\n",
    "                    if value is not None:\n",
    "                        if key == 'Tags' and isinstance(value, list):\n",
    "                            f.write(f\"[::{key}::]\\n\")\n",
    "                            for tags in value:\n",
    "                                f.write(f\"{tags}\\n\")\n",
    "\n",
    "                        elif key == 'Article' and isinstance(value, list):\n",
    "                            f.write(f\"[::{key}::]\\n\")\n",
    "                            for bodys in value:\n",
    "                                if bodys == 'SPONSORED':\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    f.write(f\"{bodys}\\n\")\n",
    "                                    f.write(\"\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"[::{key}::]\\n{value}\\n\")\n",
    "\n",
    "                            \n",
    "            with open('parsing_progress.txt', 'w') as f:\n",
    "                f.write(f'{article_id}')\n",
    "\n",
    "            with open('parsing_number.csv', 'a', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                validation = 'Valid'\n",
    "                writer.writerow([article_id, validation])\n",
    "\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article {article_id}: {e}\")\n",
    "            with open('parsing_number.csv', 'a', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                validation = 'Error'\n",
    "                writer.writerow([article_id, validation])\n",
    "                continue\n",
    "\n",
    "\n",
    "# Write the last processed article_id to 'csv_progress.txt' after loop ends\n",
    "with open('parsing_progress.txt', 'w') as f:\n",
    "    f.write(str(article_id) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50055213",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'cwd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcwd\u001b[49m()\n\u001b[0;32m      3\u001b[0m path\n\u001b[0;32m      5\u001b[0m ex_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mcwd() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../Kao\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'os' has no attribute 'cwd'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.cwd()\n",
    "path\n",
    "\n",
    "ex_path = os.cwd() + \"../../Kao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "430d3b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#start using \"csv_progress\" (current progress of parsing)\n",
    "if os.path.exists('parsing_progress.txt'):\n",
    "    with open('parsing_progress.txt','r') as f:\n",
    "        progress_start = int(f.readline().strip())\n",
    "        start_id = progress_start - 1\n",
    "else: \n",
    "    start_id = 1 \n",
    "    \n",
    "article_id = start_id\n",
    "\n",
    "\n",
    "process = 'E:/Crimson_News/src/CrawlingCode/kaohoon/progress.txt'\n",
    "\n",
    "#start using \"progress\" (last progress of crwaling)\n",
    "if os.path.exists(process):\n",
    "    with open(process,'r') as f:\n",
    "        progress_end = int(f.readline().strip())\n",
    "        end_id = progress_end + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f3f925e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612722 613046\n"
     ]
    }
   ],
   "source": [
    "print(start_id,end_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6133c929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
