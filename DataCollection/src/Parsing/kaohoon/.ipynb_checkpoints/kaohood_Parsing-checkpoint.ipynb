{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd5babdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5fb15e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#start using \"csv_progress\" (current progress of parsing)\n",
    "if os.path.exists('parsing_progress.txt'):\n",
    "    with open('parsing_progress.txt','r') as f:\n",
    "        progress_start = int(f.readline().strip())\n",
    "        start_id = progress_start - 1\n",
    "else: \n",
    "    start_id = 1 \n",
    "    \n",
    "article_id = start_id\n",
    "\n",
    "\n",
    "process = 'E:/Crimson_News/src/CrawlingCode/kaohoon/progress.txt'\n",
    "\n",
    "#start using \"progress\" (last progress of crwaling)\n",
    "if os.path.exists(process):\n",
    "    with open(process,'r') as f:\n",
    "        progress_end = int(f.readline().strip())\n",
    "        end_id = progress_end + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e4a5817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198525 198533\n"
     ]
    }
   ],
   "source": [
    "print(start_id, end_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22081c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Detect input file\n",
    "def htmlInput(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        encoding_result = chardet.detect(file.read())\n",
    "    with open(file_path, \"r\", encoding=encoding_result[\"encoding\"]) as file:\n",
    "        html_content = file.read()\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "#Extract title\n",
    "def extract_title(soup):\n",
    "    title = soup.title.string.strip() if soup.title else None\n",
    "    return title\n",
    "\n",
    "#Extract body\n",
    "def extract_body(soup):\n",
    "    article_divs = soup.find_all(\"div\", class_=\"entry-content entry clearfix\")\n",
    "    articles = []\n",
    "    if article_divs:\n",
    "        for article_div in article_divs:\n",
    "            articleses = article_div.find_all('p')\n",
    "            for article in articleses:\n",
    "                articles.append(\" \".join(article.get_text().split()))\n",
    "    \n",
    "    return articles\n",
    "\n",
    "#Extract tags\n",
    "def extract_tags(soup):\n",
    "    tagcloud_span = soup.find('span', class_='tagcloud')\n",
    "    if tagcloud_span is not None:\n",
    "        tags = tagcloud_span.find_all('a')\n",
    "        tag_content = [tag.get_text(strip=True) for tag in tags]\n",
    "        unnecessary_tag = soup.find('div', class_='post-bottom-meta post-bottom-tags post-tags-classic')\n",
    "        unnecessary_tag.decompose()\n",
    "    else:\n",
    "        tag_content = ['Tag not found']\n",
    "    return tag_content\n",
    "\n",
    "#Extract date\n",
    "def extract_pubdate(soup):\n",
    "    date_div = soup.find('div', {'id': 'single-post-meta'})\n",
    "    date_span = date_div.find('span', {'class': 'date meta-item tie-icon'})\n",
    "    timestamps = date_span.text.strip() if date_span else None\n",
    "    if timestamps is not None:\n",
    "        date_obj = datetime.datetime.strptime(timestamps, '%d/%m/%Y')\n",
    "        pubdate = date_obj.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        pubdate = None\n",
    "    return pubdate\n",
    "\n",
    "#Extract title\n",
    "def extract_intro(soup):\n",
    "    intro_div = soup.find(\"h2\", class_=\"entry-sub-title\")\n",
    "    intro = \"\"\n",
    "    if intro_div:\n",
    "        intro = intro_div.get_text(strip=True)\n",
    "    else:\n",
    "        intro = None\n",
    "\n",
    "    return intro\n",
    "\n",
    "def extract_urlPic(soup):\n",
    "    url = None\n",
    "    return url\n",
    "\n",
    "#start using \"csv_progress\" (current progress of parsing)\n",
    "if os.path.exists('parsing_progress.txt'):\n",
    "    with open('parsing_progress.txt','r') as f:\n",
    "        progress_start = int(f.readline().strip())\n",
    "        start_id = progress_start - 1\n",
    "else: \n",
    "    start_id = 1 \n",
    "    \n",
    "article_id = start_id\n",
    "\n",
    "\n",
    "process = 'E:/Crimson_News/src/CrawlingCode/kaohoon/progress.txt'\n",
    "\n",
    "#start using \"progress\" (last progress of crwaling)\n",
    "if os.path.exists(process):\n",
    "    with open(process,'r') as f:\n",
    "        progress_end = int(f.readline().strip())\n",
    "        end_id = progress_end + 1\n",
    "\n",
    "\n",
    "for article_id in range(start_id, end_id+1):\n",
    "    file_path = \"E:/Crimson_News/DataSet/kaohoon/article/\" + str(article_id) + \"/index.txt\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            soup = htmlInput(file_path)\n",
    "            title = extract_title(soup)\n",
    "            intro = extract_intro(soup)\n",
    "            articles = extract_body(soup)\n",
    "            pubdate = extract_pubdate(soup)\n",
    "            tag_content = extract_tags(soup)\n",
    "            url = extract_urlPic(soup)\n",
    "\n",
    "            data_dict = {'Title': title, 'Intro': intro, 'Article': articles, 'DateTime': pubdate, 'Tags': tag_content, 'url_picture': url}\n",
    "\n",
    "            with open(os.path.join(\"E:/Crimson_News/DataSet/kaohoon/article/\" + str(article_id) +\"/parsing.txt\"), 'w', encoding=\"utf-8\") as f:\n",
    "                for key, value in data_dict.items():\n",
    "                    if value is not None:\n",
    "                        if key == 'Tags' and isinstance(value, list):\n",
    "                            f.write(f\"[::{key}::]\\n\")\n",
    "                            for tags in value:\n",
    "                                f.write(f\"{tags}\\n\")\n",
    "\n",
    "                        elif key == 'Article' and isinstance(value, list):\n",
    "                            f.write(f\"[::{key}::]\\n\")\n",
    "                            for bodys in value:\n",
    "                                if bodys == 'SPONSORED':\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    f.write(f\"{bodys}\\n\")\n",
    "                                    f.write(\"\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"[::{key}::]\\n{value}\\n\")\n",
    "\n",
    "                            \n",
    "            with open('parsing_progress.txt', 'w') as f:\n",
    "                f.write(f'{article_id}')\n",
    "\n",
    "            with open('parsing_number.csv', 'a', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                validation = 'Valid'\n",
    "                writer.writerow([article_id, validation])\n",
    "\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article {article_id}: {e}\")\n",
    "            with open('parsing_number.csv', 'a', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                validation = 'Error'\n",
    "                writer.writerow([article_id, validation])\n",
    "                continue\n",
    "\n",
    "\n",
    "# Write the last processed article_id to 'csv_progress.txt' after loop ends\n",
    "with open('parsing_progress.txt', 'w') as f:\n",
    "    f.write(str(article_id) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d3b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
