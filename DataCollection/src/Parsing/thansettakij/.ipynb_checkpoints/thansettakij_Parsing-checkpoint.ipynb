{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f716e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import chardet\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17d145ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#start using \"csv_progress\" (current progress of parsing)\n",
    "if os.path.exists('parsing_progress.txt'):\n",
    "    with open('parsing_progress.txt','r') as f:\n",
    "        progress_start = int(f.readline().strip())\n",
    "        start_id = progress_start - 1\n",
    "else: \n",
    "    start_id = 1 \n",
    "    \n",
    "article_id = start_id\n",
    "\n",
    "\n",
    "process = 'E:/Crimson_News/src/CrawlingCode/thansettakij/progress.txt'\n",
    "\n",
    "#start using \"progress\" (last progress of crwaling)\n",
    "if os.path.exists(process):\n",
    "    with open(process,'r') as f:\n",
    "        progress_end = int(f.readline().strip())\n",
    "        end_id = progress_end + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eb270d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93405 225895\n"
     ]
    }
   ],
   "source": [
    "print(start_id, end_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c184662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index TXT Processing ID: 93406\n",
      "Index TXT Processing ID: 93412\n",
      "Index TXT Processing ID: 93415\n",
      "Index TXT Processing ID: 93419\n",
      "Index TXT Processing ID: 93422\n",
      "Index TXT Processing ID: 93423\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import chardet\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Detect input file\n",
    "def htmlInput(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        encoding_result = chardet.detect(file.read())\n",
    "    with open(file_path, \"r\", encoding=encoding_result[\"encoding\"]) as file:\n",
    "        html_content = file.read()\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "#Extract title\n",
    "def extract_title(soup):\n",
    "    title = soup.title.string.strip() if soup.title else None\n",
    "    return title\n",
    "\n",
    "#Extract body\n",
    "def extract_body(soup):\n",
    "    article_divs = soup.find_all('div', class_=\"detail\")\n",
    "    articles = []\n",
    "\n",
    "    if article_divs:\n",
    "        for article_div in article_divs:\n",
    "            content_details = article_div.find_all('div', class_='content-detail')\n",
    "            for content_detail in content_details:\n",
    "                p_tags = content_detail.find_all('p')\n",
    "                if not p_tags:\n",
    "                    parts = content_detail.stripped_strings\n",
    "                    for part in parts:\n",
    "                        articles.append(part)\n",
    "                else:\n",
    "                    for p_tag in p_tags:\n",
    "                        articles.append(p_tag.get_text(strip=True))\n",
    "\n",
    "    articles = [s.replace('\\x8d', ' ') for s in articles]\n",
    "    articles = [item for item in articles if item != '']\n",
    "\n",
    "    # Remove the caption text\n",
    "    articles = [re.sub(r'\\[caption.*?\\[/caption\\]', '', s) for s in articles]\n",
    "\n",
    "    return articles\n",
    "\n",
    "#Extract tags\n",
    "def extract_tags(soup):\n",
    "    tag_div = soup.find('div', class_='cat--title')\n",
    "    tags = []\n",
    "\n",
    "    if tag_div:\n",
    "        tag_content = tag_div.get_text(strip=True)\n",
    "    else: \n",
    "        tag_content = None\n",
    "    return tag_content\n",
    "def extract_pubdate(soup):\n",
    "    month_list = {\n",
    "        'มกราคม': '01', \n",
    "        'กุมภาพันธ์': '02', \n",
    "        'มีนาคม': '03', \n",
    "        'เมษายน': '04', \n",
    "        'พฤษภาคม': '05', \n",
    "        'มิถุนายน': '06', \n",
    "        'กรกฎาคม': '07', \n",
    "        'สิงหาคม': '08', \n",
    "        'กันยายน': '09', \n",
    "        'ตุลาคม': '10', \n",
    "        'พฤศจิกายน': '11', \n",
    "        'ธันวาคม': '12'\n",
    "    }\n",
    "\n",
    "    date_time_div = soup.find('div', class_='date')\n",
    "    pubdate = None\n",
    "\n",
    "    if date_time_div is not None:\n",
    "        date_time_string = date_time_div.get_text(strip=True)\n",
    "\n",
    "        # Split the string into words\n",
    "        words = date_time_string.split()\n",
    "\n",
    "        # Identify the components\n",
    "        day = words[0]\n",
    "        month = month_list.get(words[1])  # Get the numeric month\n",
    "        year = words[2]\n",
    "        if year.isdigit():\n",
    "            year = str(int(year) - 543)\n",
    "\n",
    "        # Form the date and time in the desired format\n",
    "        pubdate = f\"{year}-{month}-{day} \"\n",
    "\n",
    "    return pubdate\n",
    "\n",
    "#Extract title\n",
    "def extract_intro(soup):\n",
    "    intro_div = soup.find(\"h2\", class_=\"content-blurb\")\n",
    "    intro = \"\"\n",
    "    if intro_div:\n",
    "        intro = intro_div.get_text(strip=True)\n",
    "    else:\n",
    "        intro = None\n",
    "\n",
    "    return intro\n",
    "\n",
    "def extract_urlPic(soup):\n",
    "    img_div = soup.find(\"div\", class_=\"td-post-featured-image\")\n",
    "\n",
    "    if img_div is not None:\n",
    "        img_tag = img_div.find(\"img\", class_=\"entry-thumb\")\n",
    "\n",
    "        if img_tag is not None:\n",
    "            image_link = img_tag.get(\"data-cfsrc\")\n",
    "        else:\n",
    "            image_link = None\n",
    "    else:\n",
    "        image_link = None\n",
    "\n",
    "    return image_link\n",
    "\n",
    "\n",
    "\n",
    "#start using \"csv_progress\" (current progress of parsing)\n",
    "if os.path.exists('parsing_progress.txt'):\n",
    "    with open('parsing_progress.txt','r') as f:\n",
    "        progress_start = int(f.readline().strip())\n",
    "        start_id = progress_start - 1\n",
    "else: \n",
    "    start_id = 1 \n",
    "    \n",
    "article_id = start_id\n",
    "\n",
    "\n",
    "process = 'E:/Crimson_News/src/CrawlingCode/thansettakij/progress.txt'\n",
    "\n",
    "#start using \"progress\" (last progress of crwaling)\n",
    "if os.path.exists(process):\n",
    "    with open(process,'r') as f:\n",
    "        progress_end = int(f.readline().strip())\n",
    "        end_id = progress_end + 1\n",
    "\n",
    "        \n",
    "for article_id in range(start_id, end_id+1):\n",
    "    file_path = \"E:/Crimson_News/DataSet/thansettakij/article/\" + str(article_id) + \"/index.txt\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "        \n",
    "            soup = htmlInput(file_path)\n",
    "            title = extract_title(soup)\n",
    "            intro = extract_intro(soup)\n",
    "            articles = extract_body(soup)\n",
    "            pubdate = extract_pubdate(soup)\n",
    "            tag_content = extract_tags(soup)\n",
    "            url = extract_urlPic(soup)\n",
    "\n",
    "            if title == intro:\n",
    "                intro = None\n",
    "            if articles and intro == articles[0]:\n",
    "                articles.pop(0)\n",
    "            if title == articles[0]:\n",
    "                articles.pop(0)\n",
    "\n",
    "\n",
    "            data_dict = {'Title': title, 'Intro': intro, 'Article': articles, 'DateTime': pubdate, 'Tags': tag_content, 'url_picture': url}\n",
    "\n",
    "            with open(os.path.join(\"E:/Crimson_News/DataSet/thansettakij/article/\" + str(article_id) +\"/parsing.txt\"), 'w', encoding=\"utf-8\") as f:\n",
    "                for key, value in data_dict.items():\n",
    "                    if value is not None:\n",
    "                        if key == 'Tags' and isinstance(value, list):\n",
    "                            f.write(f\"[::{key}::]\\n\")\n",
    "                            for tags in value:\n",
    "                                f.write(f\"{tags}\\n\")\n",
    "\n",
    "                        elif key == 'Article' and isinstance(value, list):\n",
    "                            f.write(f\"[::{key}::]\\n\")\n",
    "                            for bodys in value:\n",
    "                                if bodys == 'SPONSORED':\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    f.write(f\"{bodys}\\n\")\n",
    "                                    f.write(\"\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"[::{key}::]\\n{value}\\n\")\n",
    "                            \n",
    "                            \n",
    "            with open('parsing_progress.txt', 'w') as f:\n",
    "                f.write(f'{article_id}')\n",
    "\n",
    "            with open('parsing_number.csv', 'a', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                validation = 'Valid'\n",
    "                writer.writerow([article_id, validation])\n",
    "\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article {article_id}: {e}\")\n",
    "            with open('parsing_number.csv', 'a', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                validation = 'Error'\n",
    "                writer.writerow([article_id, validation])\n",
    "                continue\n",
    "\n",
    "\n",
    "# Write the last processed article_id to 'csv_progress.txt' after loop ends\n",
    "with open('parsing_progress.txt', 'w') as f:\n",
    "    f.write(str(article_id) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b5d9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
