{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a5aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the directory containing the CSV files\n",
    "directory_path = '/data1-8tb-blk/sp2023stock/Insight-Wave/TopicModeling/result/integrationOutput/id/thunhoon_ids.csv'\n",
    "\n",
    "articles_list = {}\n",
    "\n",
    "df = pd.read_csv(directory_path)\n",
    "articles_list = df['ID'].tolist()\n",
    "\n",
    "print(len(articles_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8b7a264",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing article 277352: 'charmap' codec can't decode byte 0x81 in position 548: character maps to <undefined>\n",
      "Error processing article 278401: 'charmap' codec can't decode byte 0x81 in position 548: character maps to <undefined>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def htmlInput(file_path):\n",
    "    with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "#Extract title\n",
    "def extract_title(soup):\n",
    "    title = soup.title.string.strip() if soup.title else None\n",
    "    return title\n",
    "\n",
    "#Extract body\n",
    "def extract_body(soup):\n",
    "    article_divs = soup.find_all(\"div\", class_=\"article-p\")\n",
    "    articles = []\n",
    "    if article_divs:\n",
    "        for article_div in article_divs:\n",
    "            for p in article_div.find_all('p', {'style': 'text-align:right'}):  # if <div> contains <p style=\"text-align:right\">\n",
    "                p.decompose()  # remove <p> tag\n",
    "            articles.append(article_div.get_text(strip=True))\n",
    "    \n",
    "        articles = [s.replace('\\n', ' ') for s in articles]\n",
    "        articles = [s.replace('\\xa0', ' ') for s in articles]\n",
    "        articles = [item for item in articles if item != '']\n",
    "\n",
    "    \n",
    "    return articles\n",
    "\n",
    "\n",
    "#Extract tags\n",
    "def extract_tags(soup):\n",
    "    tagcloud_span = soup.find(\"h6\", class_=\"newsTagColor\")\n",
    "    \n",
    "    if tagcloud_span is not None:\n",
    "        tags = tagcloud_span.find_all('a')\n",
    "        tag_content = [tag.get_text(strip=True) for tag in tags]\n",
    "        tag_content = [item for item in tag_content if item != '']\n",
    "    else:\n",
    "        tag_content = None\n",
    "    \n",
    "    return tag_content\n",
    "\n",
    "def extract_pubdate(soup):\n",
    "    month_list = {\n",
    "        'มกราคม': '01', \n",
    "        'กุมภาพันธ์': '02', \n",
    "        'มีนาคม': '03', \n",
    "        'เมษายน': '04', \n",
    "        'พฤษภาคม': '05', \n",
    "        'มิถุนายน': '06', \n",
    "        'กรกฎาคม': '07', \n",
    "        'สิงหาคม': '08', \n",
    "        'กันยายน': '09', \n",
    "        'ตุลาคม': '10', \n",
    "        'พฤศจิกายน': '11', \n",
    "        'ธันวาคม': '12'\n",
    "    }\n",
    "    \n",
    "    # Assuming soup is already defined with your HTML content\n",
    "    date_time_div = soup.find(\"p\", class_=re.compile(\"^Article__PublishTimeStamp-sc-18ssv56-0\"))\n",
    "    \n",
    "    if date_time_div is not None:\n",
    "        date_time_string = date_time_div.get_text(strip=True)\n",
    "    \n",
    "        # Split the string into words\n",
    "        words = date_time_string.split()\n",
    "    \n",
    "        # Identify the components\n",
    "        day = words[0]\n",
    "        month = month_list.get(words[1])  # Get the numeric month\n",
    "        year = words[2]\n",
    "        time = words[4]\n",
    "    \n",
    "        # Form the date and time in the desired format\n",
    "        pubdate = f\"{year}-{month}-{day} {time}\"\n",
    "    else:\n",
    "        pubdate = None\n",
    "    \n",
    "    return pubdate\n",
    "\n",
    "#Extract title\n",
    "def extract_intro(soup):\n",
    "    intro_div = soup.find(\"h2\", class_=\"entry-sub-title\")\n",
    "    intro = \"\"\n",
    "    if intro_div:\n",
    "        intro = intro_div.get_text(strip=True)\n",
    "    else:\n",
    "        intro = None\n",
    "\n",
    "    return intro\n",
    "\n",
    "def extract_urlPic(soup):\n",
    "    # Try to find the primary image first\n",
    "    primary_div = soup.find('div', class_=\"container\")\n",
    "    if primary_div:\n",
    "        primary_img = primary_div.find('img')\n",
    "        if primary_img:\n",
    "            primary_src = primary_img.get('src')\n",
    "            # Check if the src length is less than 40\n",
    "            if len(primary_src) >= 40:\n",
    "                return primary_src\n",
    "            else:\n",
    "                # If the primary src is too short, look for the alternative\n",
    "                alternative_div = soup.find('div', class_=\"css-a1g4gz e1xtbsxb1\")\n",
    "                if alternative_div:\n",
    "                    alternative_img = alternative_div.find('img')\n",
    "                    if alternative_img:\n",
    "                        alternative_src = alternative_img.get('src')\n",
    "                        return alternative_src\n",
    "                    else:\n",
    "                        print(\"No alternative img tag found.\")\n",
    "                        return None\n",
    "                else:\n",
    "                    print(\"No alternative div with class 'css-a1g4gz e1xtbsxb1' found.\")\n",
    "                    return None\n",
    "        else:\n",
    "            print(\"No img tag found inside the primary div.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No primary div with class 'container' found.\")\n",
    "        return None\n",
    "\n",
    "# Usage\n",
    "# Assuming 'soup' has been defined and loaded with your HTML content\n",
    "image_url = extract_urlPic(soup)\n",
    "print(image_url)\n",
    "\n",
    "\n",
    "\n",
    "for article_id in range(start_id, end_id+1):\n",
    "    file_path = os.path.join(\"E:/Crimson_News/DataSet/thunhoon/article/\" + str(article_id) +\"/index.txt\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            soup = htmlInput(file_path)\n",
    "            title = extract_title(soup)\n",
    "            intro = extract_intro(soup)\n",
    "            articles = extract_body(soup)\n",
    "            pubdate = extract_pubdate(soup)\n",
    "            tag_content = extract_tags(soup)\n",
    "            url = extract_urlPic(soup)\n",
    "            \n",
    "            data_dict = {'Title': title, 'Intro': intro, 'Article': articles, 'DateTime': pubdate, 'Tags': tag_content, 'url_picture': url}\n",
    "            \n",
    "        \n",
    "            with open(os.path.join(\"E:/Crimson_News/DataSet/thunhoon/article/\" + str(article_id) +\"/parsing.txt\"), 'w', encoding=\"utf-8\") as f:\n",
    "                for key, value in data_dict.items():\n",
    "                    if value is not None:\n",
    "                        if key == 'Tags' and isinstance(value, list):\n",
    "                            f.write(f\"[::{key}::]\\n\")\n",
    "                            for tags in value:\n",
    "                                f.write(f\"{tags}\\n\")\n",
    "\n",
    "                        elif key == 'Article' and isinstance(value, list):\n",
    "                            f.write(f\"[::{key}::]\\n\")\n",
    "                            for bodys in value:\n",
    "                                if bodys == 'SPONSORED':\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    f.write(f\"{bodys}\\n\")\n",
    "                                    f.write(\"\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"[::{key}::]\\n{value}\\n\")\n",
    "\n",
    "\n",
    "            with open('parsing_progress.txt', 'w') as f:\n",
    "                f.write(f'{article_id}')\n",
    "\n",
    "            with open('parsing_number.csv', 'a', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                validation = 'Valid'\n",
    "                writer.writerow([article_id, validation])\n",
    "\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article {article_id}: {e}\")\n",
    "            with open('parsing_number.csv', 'a', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                validation = 'Error'\n",
    "                writer.writerow([article_id, validation])\n",
    "                continue\n",
    "\n",
    "\n",
    "# Write the last processed article_id to 'csv_progress.txt' after loop ends\n",
    "with open('parsing_progress.txt', 'w') as f:\n",
    "    f.write(str(article_id) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcdb5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
